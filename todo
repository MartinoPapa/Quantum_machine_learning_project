Check how the graph sampling happens
Explain the idea of sampling in model description

Explain all hyperparameters

Weighted loss for the loss of L_sup

SMall graphs have 0 for betti numbers higher than 1, bigger graphs are necessary to capture topological features

Values of epsilon for betti numbers

Jumps in the loss when frauds are found

In the algorithm there is a problem with how we define the transaction graph. We need to change that part.
The new implementation should define the edges as follows:
The edge are directed and the edge between node i and j is the sum of the amount of the transactions from i to j, vice versa the edge between node j and i will be the sum of transactions from j to i.
We do not to filter and keep only TRANSFER and CASH_OUT you can keep every type of transaction.
A big graph G containing all the transaction should be firstly captured and then, from it you should derive the subgraphs to form the traning, validation and test using random-walk based sampling.
As hyperparameters add TRAINING_DIM, VALUATION_DIM (the dimension of S_normal), TEST_DIM, EDGES (minimu number of edges in the first big graph), SAMPLED_EDGES (minum number of edges in the sampled graphs)
Everything else should be kept as it was in the previous implementation

Calculate the density of paysim and show why the algorithm is not working welll, find a way to sample nodes why higher degree